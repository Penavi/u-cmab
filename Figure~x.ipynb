{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"http://www.minethatdata.com/Kevin_Hillstrom_MineThatData_E-MailAnalytics_DataMiningChallenge_2008.03.20.csv\")\n",
    "\n",
    "data[\"history_segment\"] = pd.Categorical(data[\"history_segment\"])\n",
    "data[\"zip_code\"] = pd.Categorical(data[\"zip_code\"])\n",
    "data[\"channel\"] = pd.Categorical(data[\"channel\"])\n",
    "data[\"segment\"] = pd.Categorical(data[\"segment\"])\n",
    "\n",
    "one_hot_hs = pd.get_dummies(data[\"history_segment\"], prefix=\"hs\")\n",
    "one_hot_zc = pd.get_dummies(data[\"zip_code\"], prefix=\"zc\")\n",
    "one_hot_c = pd.get_dummies(data[\"channel\"], prefix=\"c\")\n",
    "one_hot_s = pd.get_dummies(data[\"segment\"], prefix=\"s\")\n",
    "\n",
    "data = pd.concat([data[[\"recency\", \"history\", \"mens\", \"womens\", \"newbie\", \"visit\", \"conversion\", \"spend\"]], one_hot_hs, one_hot_zc, one_hot_c, one_hot_s], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recency</th>\n",
       "      <th>history</th>\n",
       "      <th>mens</th>\n",
       "      <th>womens</th>\n",
       "      <th>newbie</th>\n",
       "      <th>visit</th>\n",
       "      <th>conversion</th>\n",
       "      <th>spend</th>\n",
       "      <th>hs_1) $0 - $100</th>\n",
       "      <th>hs_2) $100 - $200</th>\n",
       "      <th>...</th>\n",
       "      <th>hs_7) $1,000 +</th>\n",
       "      <th>zc_Rural</th>\n",
       "      <th>zc_Surburban</th>\n",
       "      <th>zc_Urban</th>\n",
       "      <th>c_Multichannel</th>\n",
       "      <th>c_Phone</th>\n",
       "      <th>c_Web</th>\n",
       "      <th>s_Mens E-Mail</th>\n",
       "      <th>s_No E-Mail</th>\n",
       "      <th>s_Womens E-Mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>142.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>329.08</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   recency  history  mens  womens  newbie  visit  conversion  spend  \\\n",
       "0       10   142.44     1       0       0      0           0    0.0   \n",
       "1        6   329.08     1       1       1      0           0    0.0   \n",
       "\n",
       "   hs_1) $0 - $100  hs_2) $100 - $200       ...         hs_7) $1,000 +  \\\n",
       "0                0                  1       ...                      0   \n",
       "1                0                  0       ...                      0   \n",
       "\n",
       "   zc_Rural  zc_Surburban  zc_Urban  c_Multichannel  c_Phone  c_Web  \\\n",
       "0         0             1         0               0        1      0   \n",
       "1         1             0         0               0        0      1   \n",
       "\n",
       "   s_Mens E-Mail  s_No E-Mail  s_Womens E-Mail  \n",
       "0              0            0                1  \n",
       "1              0            1                0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFN(nn.Module):\n",
    "    def __init__(self, context_n, treatment_n, Psi):\n",
    "        super(DFN, self).__init__()\n",
    "        \n",
    "        self.context_n = context_n\n",
    "        self.treatment_n = treatment_n\n",
    "        self.Psi = torch.tensor(Psi, dtype=torch.float)\n",
    "        \n",
    "        self.lin1 = nn.Linear(context_n, 200)\n",
    "        self.fc = nn.Linear(200, treatment_n)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = self.lin1(xb)\n",
    "        xb = F.relu(xb)\n",
    "        return self.fc(xb) - self.Psi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ucmab():\n",
    "    def __init__(self, context_n, treatment_n, Psi):\n",
    "        \"\"\"\n",
    "            context_n: int, dimension size of the contexts\n",
    "            traeatment_n: int, amount of possible treatments (including control)\n",
    "            Psi: vector (size=treatment_n), cost of every treatment (including control)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.epsilon = .1\n",
    "        self.learning_rate = .001\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.C = 10\n",
    "        \n",
    "        self.Psi = Psi\n",
    "\n",
    "        self.model = DFN(action_n, state_n, Psi)\n",
    "        self.target_model = DFN(action_n, state_n)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.opt = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss_f = nn.MSELoss()\n",
    "\n",
    "        self.action_n = action_n\n",
    "        self.state_n = state_n\n",
    "        \n",
    "    def remember(self, state, action, reward):\n",
    "        self.memory.append((state, action, reward))\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.binomial(1, max(self.epsilon, self.epsilon_min)):\n",
    "            return random.randrange(self.action_n)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "            q_val = self.model(state)\n",
    "            return q_val.argmax().item()\n",
    "    \n",
    "    def replay(self, batch_size): # this is where we train -> gradient!\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "    \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward        \n",
    "\n",
    "            actual = self.target_model(torch.tensor(state, dtype=torch.float)).data.numpy()\n",
    "            actual[0][action] = target\n",
    "\n",
    "            out = self.model(torch.tensor(state, requires_grad=True, dtype=torch.float))\n",
    "            loss = self.loss_f(out, torch.tensor(actual, requires_grad=True, dtype=torch.float))\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "    \n",
    "    def refresh_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, data):\n",
    "        self.data = data # pandas.df\n",
    "    \n",
    "    def sample(self, n):\n",
    "        s = self.data.sample(n=n)\n",
    "        return (s[])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10000\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "state_n = 18\n",
    "action_n = 2\n",
    "Psi = [.0, .3]\n",
    "agent = Ucmab(action_n, state_n, Psi)\n",
    "#done = False\n",
    "batch_size = 32\n",
    "\n",
    "windowed_treatments = []\n",
    "windowed_rewards = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_n])\n",
    "  \n",
    "    for time in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # punish terminal state?\n",
    "        next_state = np.reshape(next_state, [1, state_n])\n",
    "        agent.remember(state, action, reward, next_state, done)        \n",
    "    \n",
    "    if done:\n",
    "        print(\"episode {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, time, agent.epsilon))\n",
    "        rewards.append(time)\n",
    "        break\n",
    "  \n",
    "  if e%agent.C == 0:\n",
    "    agent.refresh_target_model()\n",
    "  \n",
    "  if len(agent.memory) > batch_size:\n",
    "    agent.replay(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL1)",
   "language": "python",
   "name": "rl1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
